PDF Q&A Assistant with Vertex AI and RAG
This project demonstrates a complete Retrieval-Augmented Generation (RAG) pipeline built with Python. It allows a user to ask questions against a private collection of PDF documents and receive accurate, source-grounded answers generated by Google's Gemini models via Vertex AI.
The system first processes a directory of PDFs, extracts the text, chunks it, and generates semantic vector embeddings for each chunk. These embeddings are stored locally in a ChromaDB vector database. When a user asks a question, the script generates an embedding for the question, retrieves the most relevant text chunks from the database, and then feeds that context along with the original question to a powerful language model to generate a final answer.
Features

- **PDF Ingestion**: Automatically processes all PDF files placed in a designated directory.
- **Text Extraction & Cleaning**: Uses PyMuPDF to efficiently extract text and cleans it for processing.
- **Semantic Chunking**: Splits text into manageable, overlapping chunks to preserve context.
- **Vector Embeddings**: Leverages Google's gemini-embedding-001 model via Vertex AI to create high-quality semantic embeddings.
- **Local Vector Storage**: Uses ChromaDB to store and query document embeddings locally.
- **RAG-Powered Q&A**: Takes a user question, retrieves relevant context from the vector database, and uses a generative model (gemini-2.5-flash-001) to provide answers based only on the provided documents.

Technology Stack
Language: Python 3.10+
Cloud Provider: Google Cloud Platform (GCP)
AI Services: Google Cloud Vertex AI
Embedding Model: gemini-embedding-001
Generation Model: gemini-2.5-flash-001
Vector Database: ChromaDB (local persistence)
PDF Processing: PyMuPDF
Setup and Installation
Follow these steps to get the project running on your local machine.

1. Clone the Repository
git clone <your-repository-url>
cd <your-repository-name>


2. Set Up a Python Virtual Environment
It is highly recommended to use a virtual environment to manage dependencies.
# Create a virtual environment
python -m venv .venv

# Activate the virtual environment

**On Windows**
.venv\Scripts\activate

**On macOS/Linux**
source .venv/bin/activate

3. Install Dependencies
Install all the required Python libraries from the requirements.txt file.
pip install -r requirements.txt


4. Google Cloud Authentication
This project requires authentication with Google Cloud to use the Vertex AI API.
Enable the Vertex AI API: Ensure the Vertex AI API is enabled in your GCP project.
Create a Service Account: Follow the GCP documentation to create a service account. Grant it the "Vertex AI User" role.
Download the Key: Generate a JSON key for your service account and download it.
Place the Key: Move the downloaded JSON key file into the root directory of this project and rename it to service_account_key.json.


5. Add Your Documents
Place all the PDF files you want to include in your knowledge base into the /pdfs directory.
Usage
The project is split into two main scripts.

# Step 1: Ingest and Embed Documents

First, you need to run the ingestion script to process your PDFs and populate the ChromaDB database.
python main_script.py This script will create a chroma_db directory in your project folder containing the vector store. You only need to run this script once, or again when you add or change the documents in the pdfs folder.

# Step 2: Ask Questions

Once the ingestion is complete, you can run the query script to start the interactive Q&A assistant.
python query_script.py The script will initialize and prompt you to ask questions. Type your question and press Enter. To exit, type quit.

# Limitations and Future Improvements
This project serves as a proof-of-concept. For a production-grade system, several areas could be enhanced:

**Advanced Chunking Strategy**
The current implementation uses a fixed-size character chunking with overlap. While effective, this can sometimes split sentences or ideas in the middle, potentially reducing the semantic quality of a chunk. A more advanced solution could implement:

Recursive Character Text Splitting: A method that tries to split text along a hierarchy of separators (e.g., paragraphs, then sentences, then words) to keep related text together.

Semantic Chunking: Using NLP models to divide text into chunks based on topical coherence, ensuring each chunk represents a complete idea.

**Sophisticated Search and Retrieval**
ChromaDB is an excellent tool for local development and prototyping. For a scalable, production environment, one might consider:

Managed Vector Databases: Services like Google's Vertex AI Vector Search or other third-party solutions (e.g., Pinecone, Weaviate) offer better scalability, reliability, and performance for very large datasets.

Hybrid Search: Combining traditional keyword-based search (like BM25) with semantic vector search can improve retrieval accuracy, especially for queries containing specific codes or proper nouns.

Re-ranking: A more complex pipeline could retrieve a larger number of initial documents (e.g., top 20) and then use a secondary, more sophisticated model (a re-ranker) to select the absolute best results to feed into the final generation prompt.

Security Note
IMPORTANT: The .gitignore file is configured to ignore the service_account_key.json file, the /pdfs directory, and the /chroma_db directory. Do not commit these files to your public GitHub repository to avoid exposing your private credentials and documents.
